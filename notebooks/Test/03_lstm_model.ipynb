{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67276af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:16.487917Z",
     "iopub.status.busy": "2025-09-21T20:47:16.487671Z",
     "iopub.status.idle": "2025-09-21T20:47:17.841380Z",
     "shell.execute_reply": "2025-09-21T20:47:17.840784Z"
    },
    "papermill": {
     "duration": 1.359575,
     "end_time": "2025-09-21T20:47:17.842896",
     "exception": false,
     "start_time": "2025-09-21T20:47:16.483321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "ROOT = Path(\"/kaggle/input/log-data-for-anomaly-detection/Hadoop_log/Hadoop_log\")\n",
    "LABEL_FILE = ROOT / \"abnormal_label.txt\"\n",
    "OUT_DIR = Path(\"/kaggle/working\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sampling\n",
    "SAMPLES_PER_LABEL = 15\n",
    "SAMPLES_PER_APP = 10\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec76e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:17.850776Z",
     "iopub.status.busy": "2025-09-21T20:47:17.850087Z",
     "iopub.status.idle": "2025-09-21T20:47:17.857930Z",
     "shell.execute_reply": "2025-09-21T20:47:17.857445Z"
    },
    "papermill": {
     "duration": 0.01252,
     "end_time": "2025-09-21T20:47:17.858962",
     "exception": false,
     "start_time": "2025-09-21T20:47:17.846442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_labels(label_file: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parse abnormal_label.txt of the form:\n",
    "        abnormal:\n",
    "        +application_123\n",
    "        +application_456\n",
    "        normal:\n",
    "        +application_789\n",
    "    Returns dict: { 'application_123': 'abnormal', ... }\n",
    "    \"\"\"\n",
    "    app_label = {}\n",
    "    current_label = None\n",
    "    if not label_file.exists():\n",
    "        print(f\"[WARN] Label file not found: {label_file}\")\n",
    "        return app_label\n",
    "\n",
    "    with label_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.endswith(\":\"):\n",
    "                current_label = line[:-1].strip()\n",
    "                continue\n",
    "            if line.startswith(\"+\"):\n",
    "                if current_label is None:\n",
    "                    # In case of malformed ordering\n",
    "                    continue\n",
    "                app = line[1:].strip()\n",
    "                if app:\n",
    "                    app_label[app] = current_label\n",
    "    return app_label\n",
    "\n",
    "\n",
    "def safe_count_lines(file_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Stream file to count lines without loading into memory.\n",
    "    Handles potential encoding issues by ignoring errors.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def stream_lines(file_path: Path, max_lines=None):\n",
    "    \"\"\"\n",
    "    Yield lines from a text file with robust decoding.\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line:\n",
    "                yield line\n",
    "            n += 1\n",
    "            if max_lines is not None and n >= max_lines:\n",
    "                break\n",
    "\n",
    "\n",
    "def reservoir_sample(iterable, k, seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Classic reservoir sampling to draw k items from an iterator without reading all into memory.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    sample = []\n",
    "    for i, item in enumerate(iterable, start=1):\n",
    "        if i <= k:\n",
    "            sample.append(item)\n",
    "        else:\n",
    "            j = random.randint(1, i)\n",
    "            if j <= k:\n",
    "                sample[j-1] = item\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5aaa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:17.865503Z",
     "iopub.status.busy": "2025-09-21T20:47:17.865289Z",
     "iopub.status.idle": "2025-09-21T20:47:28.237970Z",
     "shell.execute_reply": "2025-09-21T20:47:28.237184Z"
    },
    "papermill": {
     "duration": 10.377802,
     "end_time": "2025-09-21T20:47:28.239449",
     "exception": false,
     "start_time": "2025-09-21T20:47:17.861647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Hadoop Log Dataset – Quick Report\n",
      "\n",
      "- Root: `/kaggle/input/log-data-for-anomaly-detection/Hadoop_log/Hadoop_log`\n",
      "- Applications found: **55**\n",
      "- Log files: **978**\n",
      "- Total log lines (approx.): **394,310**\n",
      "- Labels in label file: ['Disk full', 'Machine down', 'Network disconnection', 'Normal']\n",
      "- Applications without a label (`unknown`): **0**\n",
      "\n",
      "## Label Distribution (by applications)\n",
      "| label                 |   num_apps |   total_lines |\n",
      "|:----------------------|-----------:|--------------:|\n",
      "| Disk full             |          9 |         27946 |\n",
      "| Machine down          |         28 |        102960 |\n",
      "| Network disconnection |          7 |        237979 |\n",
      "| Normal                |         11 |         25425 |\n",
      "\n",
      "\n",
      "Wrote full report to: /kaggle/working/report.md\n",
      "Artifacts:\n",
      " - /kaggle/working/file_inventory.csv\n",
      " - /kaggle/working/app_stats.csv\n",
      " - /kaggle/working/label_counts.csv\n",
      " - /kaggle/working/sample_logs.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Parse labels\n",
    "    app_to_label = parse_labels(LABEL_FILE)\n",
    "\n",
    "    # 2) Inventory: list application_* dirs and their *.log files\n",
    "    apps = []\n",
    "    file_rows = []   # for file_inventory.csv\n",
    "    app_stats = defaultdict(lambda: {\"num_files\": 0, \"total_lines\": 0, \"label\": \"unknown\"})\n",
    "\n",
    "    if not ROOT.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {ROOT}\")\n",
    "\n",
    "    for entry in sorted(ROOT.iterdir()):\n",
    "        if entry.is_dir() and entry.name.startswith(\"application_\"):\n",
    "            app_name = entry.name\n",
    "            label = app_to_label.get(app_name, \"unknown\")\n",
    "            apps.append(app_name)\n",
    "\n",
    "            log_files = sorted([p for p in entry.iterdir() if p.is_file() and p.suffix == \".log\"])\n",
    "            for lf in log_files:\n",
    "                # Count size and lines\n",
    "                size_bytes = lf.stat().st_size\n",
    "                line_count = safe_count_lines(lf)\n",
    "\n",
    "                file_rows.append({\n",
    "                    \"application\": app_name,\n",
    "                    \"label\": label,\n",
    "                    \"file_path\": str(lf),\n",
    "                    \"file_name\": lf.name,\n",
    "                    \"size_bytes\": size_bytes,\n",
    "                    \"line_count\": line_count,\n",
    "                })\n",
    "\n",
    "                # Accumulate into app_stats\n",
    "                app_stats[app_name][\"num_files\"] += 1\n",
    "                app_stats[app_name][\"total_lines\"] += line_count\n",
    "                app_stats[app_name][\"label\"] = label\n",
    "\n",
    "    # 3) Convert to DataFrames and persist\n",
    "    inv_df = pd.DataFrame(file_rows).sort_values([\"application\", \"file_name\"])\n",
    "    inv_path = OUT_DIR / \"file_inventory.csv\"\n",
    "    inv_df.to_csv(inv_path, index=False)\n",
    "\n",
    "    app_df = (\n",
    "        pd.DataFrame([\n",
    "            {\"application\": app, **vals}\n",
    "            for app, vals in app_stats.items()\n",
    "        ])\n",
    "        .sort_values([\"label\", \"total_lines\"], ascending=[True, False])\n",
    "    )\n",
    "    app_path = OUT_DIR / \"app_stats.csv\"\n",
    "    app_df.to_csv(app_path, index=False)\n",
    "\n",
    "    # 4) Label distribution (applications and lines)\n",
    "    label_counts_apps = app_df[\"label\"].value_counts().rename_axis(\"label\").reset_index(name=\"num_apps\")\n",
    "    # lines per label\n",
    "    lines_per_label = app_df.groupby(\"label\")[\"total_lines\"].sum().reset_index(name=\"total_lines\")\n",
    "    label_summary = pd.merge(label_counts_apps, lines_per_label, on=\"label\", how=\"outer\").fillna(0)\n",
    "    label_path = OUT_DIR / \"label_counts.csv\"\n",
    "    label_summary.to_csv(label_path, index=False)\n",
    "\n",
    "    # 5) Sampling: a) per-label sample, b) per-app sample\n",
    "    #    We’ll write a compact JSONL with entries:\n",
    "    #    {\"kind\": \"per_label\", \"label\": ..., \"application\": ..., \"file\": ..., \"line\": \"...\"}\n",
    "    #    {\"kind\": \"per_app\", \"application\": ..., \"label\": ..., \"file\": ..., \"line\": \"...\"}\n",
    "    samples_path = OUT_DIR / \"sample_logs.jsonl\"\n",
    "    with samples_path.open(\"w\", encoding=\"utf-8\") as jout:\n",
    "        # a) Per-label samples\n",
    "        for label in sorted(label_summary[\"label\"].unique()):\n",
    "            # Gather all lines for this label via reservoir sample across all its files\n",
    "            # To keep it efficient, we sample per file then re-sample globally.\n",
    "            per_file_samples = []\n",
    "            for _, row in inv_df[inv_df[\"label\"] == label].iterrows():\n",
    "                lines = reservoir_sample(stream_lines(Path(row[\"file_path\"])), k=3)  # small per-file sample\n",
    "                for ln in lines:\n",
    "                    per_file_samples.append({\n",
    "                        \"kind\": \"per_label\",\n",
    "                        \"label\": label,\n",
    "                        \"application\": row[\"application\"],\n",
    "                        \"file\": row[\"file_name\"],\n",
    "                        \"line\": ln\n",
    "                    })\n",
    "            # Re-sample to SAMPLES_PER_LABEL\n",
    "            selected = reservoir_sample((x for x in per_file_samples), k=min(SAMPLES_PER_LABEL, max(1, len(per_file_samples))))\n",
    "            for item in selected:\n",
    "                jout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # b) Per-app samples\n",
    "        for app_name in app_df[\"application\"]:\n",
    "            label = app_stats[app_name][\"label\"]\n",
    "            per_app_lines = []\n",
    "            rows = inv_df[inv_df[\"application\"] == app_name]\n",
    "            for _, row in rows.iterrows():\n",
    "                lines = reservoir_sample(stream_lines(Path(row[\"file_path\"])), k=2)  # tiny per-file for spread\n",
    "                for ln in lines:\n",
    "                    per_app_lines.append({\n",
    "                        \"kind\": \"per_app\",\n",
    "                        \"application\": app_name,\n",
    "                        \"label\": label,\n",
    "                        \"file\": row[\"file_name\"],\n",
    "                        \"line\": ln\n",
    "                    })\n",
    "            selected = reservoir_sample((x for x in per_app_lines), k=min(SAMPLES_PER_APP, max(1, len(per_app_lines))))\n",
    "            for item in selected:\n",
    "                jout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # 6) Quick textual report\n",
    "    total_apps = len(app_df)\n",
    "    total_files = len(inv_df)\n",
    "    total_lines = int(inv_df[\"line_count\"].sum()) if not inv_df.empty else 0\n",
    "    known_labels = sorted(set(app_to_label.values()))\n",
    "    unknown_apps = int((app_df[\"label\"] == \"unknown\").sum())\n",
    "\n",
    "    report_lines = []\n",
    "    report_lines.append(\"# Hadoop Log Dataset – Quick Report\")\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(f\"- Root: `{ROOT}`\")\n",
    "    report_lines.append(f\"- Applications found: **{total_apps}**\")\n",
    "    report_lines.append(f\"- Log files: **{total_files}**\")\n",
    "    report_lines.append(f\"- Total log lines (approx.): **{total_lines:,}**\")\n",
    "    report_lines.append(f\"- Labels in label file: {known_labels if known_labels else 'None found'}\")\n",
    "    report_lines.append(f\"- Applications without a label (`unknown`): **{unknown_apps}**\")\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"## Label Distribution (by applications)\")\n",
    "    report_lines.append(label_summary.to_markdown(index=False))\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"## Top Applications by Line Count\")\n",
    "    top_apps = app_df.nlargest(10, \"total_lines\")[[\"application\", \"label\", \"num_files\", \"total_lines\"]]\n",
    "    report_lines.append(top_apps.to_markdown(index=False))\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"## Outputs\")\n",
    "    report_lines.append(f\"- `file_inventory.csv` – {inv_path}\")\n",
    "    report_lines.append(f\"- `app_stats.csv` – {app_path}\")\n",
    "    report_lines.append(f\"- `label_counts.csv` – {label_path}\")\n",
    "    report_lines.append(f\"- `sample_logs.jsonl` – {samples_path}\")\n",
    "\n",
    "    rpt_path = OUT_DIR / \"report.md\"\n",
    "    with rpt_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    # 7) Print a concise on-screen summary\n",
    "    print(\"\\n\".join(report_lines[:12]))  # print header + high-level stats\n",
    "    print(\"\\nWrote full report to:\", rpt_path)\n",
    "    print(\"Artifacts:\")\n",
    "    print(\" -\", inv_path)\n",
    "    print(\" -\", app_path)\n",
    "    print(\" -\", label_path)\n",
    "    print(\" -\", samples_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210d5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:28.246482Z",
     "iopub.status.busy": "2025-09-21T20:47:28.246239Z",
     "iopub.status.idle": "2025-09-21T20:47:41.914106Z",
     "shell.execute_reply": "2025-09-21T20:47:41.913317Z"
    },
    "papermill": {
     "duration": 13.673023,
     "end_time": "2025-09-21T20:47:41.915605",
     "exception": false,
     "start_time": "2025-09-21T20:47:28.242582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 20:47:29.781566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758487649.975964      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758487650.031532      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, optimizers, regularizers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52eec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:41.922527Z",
     "iopub.status.busy": "2025-09-21T20:47:41.922059Z",
     "iopub.status.idle": "2025-09-21T20:47:41.927956Z",
     "shell.execute_reply": "2025-09-21T20:47:41.927434Z"
    },
    "papermill": {
     "duration": 0.010274,
     "end_time": "2025-09-21T20:47:41.928913",
     "exception": false,
     "start_time": "2025-09-21T20:47:41.918639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = Path(\"/kaggle/input/log-data-for-anomaly-detection/Hadoop_log/Hadoop_log\")\n",
    "LABEL_FILE = ROOT / \"abnormal_label.txt\"\n",
    "OUT_DIR = Path(\"/kaggle/working\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TF_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.keras.utils.set_random_seed(TF_SEED)\n",
    "\n",
    "# Balanced caps across labels; modest at first, scale up once stable\n",
    "MAX_LINES_PER_LABEL_TRAIN = 30000\n",
    "MAX_LINES_PER_LABEL_VAL   = 5000\n",
    "MAX_LINES_PER_LABEL_TEST  = 5000\n",
    "\n",
    "# Cap per application to avoid any single app dominating\n",
    "MAX_LINES_PER_APP_TRAIN   = 4000\n",
    "MAX_LINES_PER_APP_VAL     = 1500\n",
    "MAX_LINES_PER_APP_TEST    = 1500\n",
    "\n",
    "# Limit proportion of low-priority (\"INFO-like\") lines per app in each split\n",
    "MAX_INFO_RATIO = 0.30  # at most 30% of sampled lines per app can be low-priority\n",
    "\n",
    "# Text vectorization (character-level)\n",
    "SEQ_LEN = 512      # truncate/pad each line to this many characters\n",
    "VOCAB = None       # None => adapt from train; or pass a fixed character list\n",
    "\n",
    "# Model / training\n",
    "EMBED_DIM = 64\n",
    "DROPOUT = 0.35\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 12\n",
    "BASE_LR = 1e-3     # lower than before for stability\n",
    "\n",
    "# Confidence floor for app-level aggregation (exclude low-confidence lines)\n",
    "CONF_FLOOR = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a262a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:41.935373Z",
     "iopub.status.busy": "2025-09-21T20:47:41.935158Z",
     "iopub.status.idle": "2025-09-21T20:47:41.947418Z",
     "shell.execute_reply": "2025-09-21T20:47:41.946864Z"
    },
    "papermill": {
     "duration": 0.016802,
     "end_time": "2025-09-21T20:47:41.948486",
     "exception": false,
     "start_time": "2025-09-21T20:47:41.931684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_labels(label_file: Path) -> dict:\n",
    "    \"\"\"Return dict: application -> label\"\"\"\n",
    "    app_to_label = {}\n",
    "    current_label = None\n",
    "    with label_file.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.endswith(\":\"):\n",
    "                current_label = line[:-1].strip()\n",
    "                continue\n",
    "            if line.startswith(\"+\") and current_label is not None:\n",
    "                app = line[1:].strip()\n",
    "                if app:\n",
    "                    app_to_label[app] = current_label\n",
    "    return app_to_label\n",
    "\n",
    "def iter_log_lines(file_path: Path):\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if ln:\n",
    "                yield ln\n",
    "\n",
    "def stratified_app_split(app_to_label, train_size=0.7, val_size=0.15, test_size=0.15, seed=RANDOM_SEED):\n",
    "    apps = np.array(sorted(app_to_label.keys()))\n",
    "    y = np.array([app_to_label[a] for a in apps])\n",
    "\n",
    "    # First split train vs (val+test)\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, train_size=train_size, random_state=seed)\n",
    "    train_idx, vt_idx = next(sss1.split(apps, y))\n",
    "\n",
    "    apps_train = apps[train_idx]\n",
    "    y_vt = y[vt_idx]\n",
    "\n",
    "    # Split remaining into val and test with preserved stratification\n",
    "    vt_apps = apps[vt_idx]\n",
    "    test_ratio_in_vt = test_size / (val_size + test_size)\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio_in_vt, random_state=seed+1)\n",
    "    val_idx_rel, test_idx_rel = next(sss2.split(vt_apps, y_vt))\n",
    "\n",
    "    apps_val  = vt_apps[val_idx_rel]\n",
    "    apps_test = vt_apps[test_idx_rel]\n",
    "    return set(apps_train), set(apps_val), set(apps_test)\n",
    "\n",
    "# Priority heuristics for sampling\n",
    "ERR_PAT  = re.compile(r\"(ERROR|FATAL|EXCEPTION|fail(ed)?|refused|timeout|disk|full|network|connect|lost|down)\", re.I)\n",
    "WARN_PAT = re.compile(r\"(WARN|warning|retry|backoff|throttle)\", re.I)\n",
    "\n",
    "def line_priority(line: str) -> float:\n",
    "    if ERR_PAT.search(line):  return 3.0\n",
    "    if WARN_PAT.search(line): return 1.5\n",
    "    return 1.0  # info/other\n",
    "\n",
    "def _collect_one_split(apps_subset, app_to_label, per_label_cap, per_app_cap, max_info_ratio):\n",
    "    \"\"\"Priority-based sampler with per-label and per-app caps, returns (texts, labels, app_to_indices).\"\"\"\n",
    "    per_label_counts = Counter()\n",
    "    texts, labels = [], []\n",
    "    app_to_indices = defaultdict(list)\n",
    "\n",
    "    apps_list = list(apps_subset)\n",
    "    random.shuffle(apps_list)\n",
    "\n",
    "    for app in apps_list:\n",
    "        lab = app_to_label[app]\n",
    "        app_dir = ROOT / app\n",
    "        if not app_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Gather and score lines for this app\n",
    "        scored = []\n",
    "        for lf in sorted([p for p in app_dir.iterdir() if p.is_file() and p.suffix == \".log\"]):\n",
    "            for ln in iter_log_lines(lf):\n",
    "                scored.append((line_priority(ln), ln))\n",
    "\n",
    "        if not scored:\n",
    "            continue\n",
    "\n",
    "        # Sort by score, prefer high priority\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Enforce info ratio within per_app_cap\n",
    "        chosen, info_count = [], 0\n",
    "        info_limit = int(max_info_ratio * per_app_cap) if per_app_cap > 0 else 0\n",
    "        for sc, ln in scored:\n",
    "            if len(chosen) >= per_app_cap:\n",
    "                break\n",
    "            is_info = (sc == 1.0)\n",
    "            if is_info and info_count >= info_limit:\n",
    "                continue\n",
    "            chosen.append(ln)\n",
    "            if is_info:\n",
    "                info_count += 1\n",
    "\n",
    "        # Add chosen to global buffers subject to per-label cap\n",
    "        for ln in chosen:\n",
    "            if per_label_counts[lab] >= per_label_cap:\n",
    "                break\n",
    "            idx = len(texts)\n",
    "            texts.append(ln)\n",
    "            labels.append(lab)\n",
    "            app_to_indices[app].append(idx)\n",
    "            per_label_counts[lab] += 1\n",
    "\n",
    "    return texts, labels, app_to_indices\n",
    "\n",
    "def collect_lines_for_split(apps_subset, app_to_label, per_label_cap, per_app_cap, max_info_ratio=MAX_INFO_RATIO):\n",
    "    return _collect_one_split(apps_subset, app_to_label, per_label_cap, per_app_cap, max_info_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82a034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:41.955061Z",
     "iopub.status.busy": "2025-09-21T20:47:41.954549Z",
     "iopub.status.idle": "2025-09-21T20:47:52.491531Z",
     "shell.execute_reply": "2025-09-21T20:47:52.490665Z"
    },
    "papermill": {
     "duration": 10.541784,
     "end_time": "2025-09-21T20:47:52.492918",
     "exception": false,
     "start_time": "2025-09-21T20:47:41.951134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Disk full', 'Machine down', 'Network disconnection', 'Normal']\n",
      "Apps per label: Counter({'Machine down': 28, 'Normal': 11, 'Disk full': 9, 'Network disconnection': 7})\n",
      "Train apps: 38, Val apps: 8, Test apps: 9\n",
      "Line counts: train 57981 val 5289 test 5871\n",
      "Classes: ['Disk full', 'Machine down', 'Network disconnection', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app_to_label = parse_labels(LABEL_FILE)\n",
    "labels_set = sorted(set(app_to_label.values()))\n",
    "print(\"Labels:\", labels_set)\n",
    "print(\"Apps per label:\", Counter(app_to_label.values()))\n",
    "\n",
    "apps_train, apps_val, apps_test = stratified_app_split(app_to_label)\n",
    "print(f\"Train apps: {len(apps_train)}, Val apps: {len(apps_val)}, Test apps: {len(apps_test)}\")\n",
    "\n",
    "X_train_lines, y_train_labels, appidx_train = collect_lines_for_split(\n",
    "    apps_train, app_to_label,\n",
    "    per_label_cap=MAX_LINES_PER_LABEL_TRAIN, per_app_cap=MAX_LINES_PER_APP_TRAIN\n",
    ")\n",
    "X_val_lines, y_val_labels, appidx_val = collect_lines_for_split(\n",
    "    apps_val, app_to_label,\n",
    "    per_label_cap=MAX_LINES_PER_LABEL_VAL, per_app_cap=MAX_LINES_PER_APP_VAL\n",
    ")\n",
    "X_test_lines, y_test_labels, appidx_test = collect_lines_for_split(\n",
    "    apps_test, app_to_label,\n",
    "    per_label_cap=MAX_LINES_PER_LABEL_TEST, per_app_cap=MAX_LINES_PER_APP_TEST\n",
    ")\n",
    "\n",
    "print(\"Line counts:\",\n",
    "      \"train\", len(X_train_lines),\n",
    "      \"val\", len(X_val_lines),\n",
    "      \"test\", len(X_test_lines))\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(list(labels_set))  # ensure stable class order\n",
    "y_train = le.transform(y_train_labels)\n",
    "y_val   = le.transform(y_val_labels)\n",
    "y_test  = le.transform(y_test_labels)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n",
    "\n",
    "# Save per-split label counts actually used\n",
    "pd.Series(Counter(y_train_labels)).to_csv(OUT_DIR / \"train_label_counts.csv\")\n",
    "pd.Series(Counter(y_val_labels)).to_csv(OUT_DIR / \"val_label_counts.csv\")\n",
    "pd.Series(Counter(y_test_labels)).to_csv(OUT_DIR / \"test_label_counts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fb0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:52.502102Z",
     "iopub.status.busy": "2025-09-21T20:47:52.501870Z",
     "iopub.status.idle": "2025-09-21T20:47:58.374011Z",
     "shell.execute_reply": "2025-09-21T20:47:58.373168Z"
    },
    "papermill": {
     "duration": 5.877411,
     "end_time": "2025-09-21T20:47:58.375563",
     "exception": false,
     "start_time": "2025-09-21T20:47:52.498152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758487673.484357      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vec = layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    split=\"character\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LEN,\n",
    "    vocabulary=VOCAB\n",
    ")\n",
    "vec.adapt(tf.data.Dataset.from_tensor_slices(np.array(X_train_lines, dtype=object)).batch(2048))\n",
    "vocab = vec.get_vocabulary()\n",
    "with open(OUT_DIR / \"char_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in vocab:\n",
    "        f.write(tok + \"\\n\")\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# -------------------\n",
    "# tf.data Pipelines\n",
    "# -------------------\n",
    "def make_ds(texts, labels, batch_size, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((np.array(texts, dtype=object), np.array(labels, dtype=np.int64)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(texts), 100000), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def vectorize_batch(text, label):\n",
    "    return vec(text), label\n",
    "\n",
    "train_ds = make_ds(X_train_lines, y_train, BATCH_SIZE, shuffle=True).map(vectorize_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds   = make_ds(X_val_lines,   y_val,   BATCH_SIZE, shuffle=False).map(vectorize_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds  = make_ds(X_test_lines,  y_test,  BATCH_SIZE, shuffle=False).map(vectorize_batch, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37571a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:58.383051Z",
     "iopub.status.busy": "2025-09-21T20:47:58.382419Z",
     "iopub.status.idle": "2025-09-21T20:47:59.297804Z",
     "shell.execute_reply": "2025-09-21T20:47:59.297212Z"
    },
    "papermill": {
     "duration": 0.920087,
     "end_time": "2025-09-21T20:47:59.298920",
     "exception": false,
     "start_time": "2025-09-21T20:47:58.378833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv1d' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv1d_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv1d_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,696</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,528</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,816</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,104</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">236,160</span> │ spatial_dropout1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m5,696\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │     \u001b[38;5;34m18,528\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │     \u001b[38;5;34m30,816\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │     \u001b[38;5;34m43,104\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m288\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m288\u001b[0m)  │      \u001b[38;5;34m1,152\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m288\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m288\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m160\u001b[0m)  │    \u001b[38;5;34m236,160\u001b[0m │ spatial_dropout1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m20,608\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m516\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,580</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,580\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,004</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,004\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> (2.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576\u001b[0m (2.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def build_model(vocab_size, num_classes):\n",
    "    inputs = layers.Input(shape=(SEQ_LEN,), dtype=tf.int64)\n",
    "    x = layers.Embedding(vocab_size, EMBED_DIM, mask_zero=True,\n",
    "                         embeddings_regularizer=regularizers.l2(1e-6))(inputs)\n",
    "\n",
    "    # Multi-kernel CNN block for varied n-grams\n",
    "    b1 = layers.Conv1D(96, 3, padding=\"same\", activation=\"relu\",\n",
    "                       kernel_regularizer=regularizers.l2(1e-6))(x)\n",
    "    b2 = layers.Conv1D(96, 5, padding=\"same\", activation=\"relu\",\n",
    "                       kernel_regularizer=regularizers.l2(1e-6))(x)\n",
    "    b3 = layers.Conv1D(96, 7, padding=\"same\", activation=\"relu\",\n",
    "                       kernel_regularizer=regularizers.l2(1e-6))(x)\n",
    "    x = layers.Concatenate()([b1, b2, b3])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.SpatialDropout1D(0.25)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(80, return_sequences=True,\n",
    "                                         dropout=0.2,\n",
    "                                         kernel_regularizer=regularizers.l2(1e-6)))(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-6))(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=len(vocab), num_classes=num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b05c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:47:59.307351Z",
     "iopub.status.busy": "2025-09-21T20:47:59.307113Z",
     "iopub.status.idle": "2025-09-21T20:50:37.625931Z",
     "shell.execute_reply": "2025-09-21T20:50:37.625177Z"
    },
    "papermill": {
     "duration": 158.324408,
     "end_time": "2025-09-21T20:50:37.627231",
     "exception": false,
     "start_time": "2025-09-21T20:47:59.302823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758487685.716605      56 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 191ms/step - accuracy: 0.6389 - loss: 0.9130 - val_accuracy: 0.1526 - val_loss: 1.4670 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 188ms/step - accuracy: 0.9203 - loss: 0.2268 - val_accuracy: 0.1579 - val_loss: 1.3863 - learning_rate: 0.0010\n",
      "Epoch 3/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - accuracy: 0.9484 - loss: 0.1454 - val_accuracy: 0.5224 - val_loss: 1.0564 - learning_rate: 0.0010\n",
      "Epoch 4/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - accuracy: 0.9549 - loss: 0.1286 - val_accuracy: 0.6563 - val_loss: 0.7988 - learning_rate: 0.0010\n",
      "Epoch 5/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9588 - loss: 0.1199\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - accuracy: 0.9588 - loss: 0.1199 - val_accuracy: 0.6542 - val_loss: 1.2940 - learning_rate: 0.0010\n",
      "Epoch 6/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9615 - loss: 0.1106\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 181ms/step - accuracy: 0.9615 - loss: 0.1105 - val_accuracy: 0.6816 - val_loss: 1.6866 - learning_rate: 5.0000e-04\n",
      "Epoch 7/12\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9648 - loss: 0.1008\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 181ms/step - accuracy: 0.9648 - loss: 0.1008 - val_accuracy: 0.6742 - val_loss: 1.9219 - learning_rate: 2.5000e-04\n",
      "\n",
      "[Line-level] Accuracy: 0.9521  Macro-F1: 0.9146\n",
      "\n",
      "[Line-level] Classification Report\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Disk full       0.96      0.99      0.98      1042\n",
      "         Machine down       0.93      0.99      0.96      2775\n",
      "Network disconnection       0.99      0.99      0.99      1500\n",
      "               Normal       0.97      0.59      0.73       554\n",
      "\n",
      "             accuracy                           0.95      5871\n",
      "            macro avg       0.96      0.89      0.91      5871\n",
      "         weighted avg       0.95      0.95      0.95      5871\n",
      "\n",
      "\n",
      "[Line-level] Confusion Matrix\n",
      "                             pred_Disk full  pred_Machine down  \\\n",
      "true_Disk full                        1036                  5   \n",
      "true_Machine down                       25               2739   \n",
      "true_Network disconnection               1                  9   \n",
      "true_Normal                             17                202   \n",
      "\n",
      "                            pred_Network disconnection  pred_Normal  \n",
      "true_Disk full                                       0            1  \n",
      "true_Machine down                                    3            8  \n",
      "true_Network disconnection                        1488            2  \n",
      "true_Normal                                          8          327  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = optimizers.Adam(learning_rate=BASE_LR)\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "cb = [\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-5, verbose=1),\n",
    "    callbacks.ModelCheckpoint(OUT_DIR / \"line_charcnn_lstm.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "# -------------------\n",
    "# Train\n",
    "# -------------------\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Evaluation: line-level\n",
    "# -------------------\n",
    "y_pred_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "print(\"\\n[Line-level] Accuracy: %.4f  Macro-F1: %.4f\" %\n",
    "      (accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"\\n[Line-level] Classification Report\\n\",\n",
    "      classification_report(y_test, y_pred, target_names=list(le.classes_)))\n",
    "print(\"\\n[Line-level] Confusion Matrix\\n\",\n",
    "      pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                   index=[f\"true_{c}\" for c in le.classes_],\n",
    "                   columns=[f\"pred_{c}\" for c in le.classes_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eeb512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:50:37.701982Z",
     "iopub.status.busy": "2025-09-21T20:50:37.701725Z",
     "iopub.status.idle": "2025-09-21T20:50:37.744792Z",
     "shell.execute_reply": "2025-09-21T20:50:37.744054Z"
    },
    "papermill": {
     "duration": 0.080517,
     "end_time": "2025-09-21T20:50:37.745884",
     "exception": false,
     "start_time": "2025-09-21T20:50:37.665367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[App-level from mean prob] Accuracy: 1.0000  Macro-F1: 1.0000\n",
      "\n",
      "[App-level] Classification Report\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Disk full       1.00      1.00      1.00         2\n",
      "         Machine down       1.00      1.00      1.00         5\n",
      "Network disconnection       1.00      1.00      1.00         1\n",
      "               Normal       1.00      1.00      1.00         1\n",
      "\n",
      "             accuracy                           1.00         9\n",
      "            macro avg       1.00      1.00      1.00         9\n",
      "         weighted avg       1.00      1.00      1.00         9\n",
      "\n",
      "\n",
      "[App-level] Confusion Matrix\n",
      "                             pred_Disk full  pred_Machine down  \\\n",
      "true_Disk full                           2                  0   \n",
      "true_Machine down                        0                  5   \n",
      "true_Network disconnection               0                  0   \n",
      "true_Normal                              0                  0   \n",
      "\n",
      "                            pred_Network disconnection  pred_Normal  \n",
      "true_Disk full                                       0            0  \n",
      "true_Machine down                                    0            0  \n",
      "true_Network disconnection                           1            0  \n",
      "true_Normal                                          0            1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_to_app = [None] * len(X_test_lines)\n",
    "for app, indices in appidx_test.items():\n",
    "    for idx in indices:\n",
    "        if 0 <= idx < len(idx_to_app):\n",
    "            idx_to_app[idx] = app\n",
    "\n",
    "app_true = {}\n",
    "app_probs = defaultdict(list)\n",
    "\n",
    "for i, app in enumerate(idx_to_app):\n",
    "    if app is None:\n",
    "        continue\n",
    "    app_true.setdefault(app, y_test[i])\n",
    "    # keep only reasonably confident lines (tune CONF_FLOOR 0.4–0.6)\n",
    "    if float(np.max(y_pred_prob[i])) >= CONF_FLOOR:\n",
    "        app_probs[app].append(y_pred_prob[i])\n",
    "\n",
    "app_level_true, app_level_pred = [], []\n",
    "for app in app_true.keys():\n",
    "    probs = app_probs.get(app, None)\n",
    "    if not probs:  # if all lines filtered, fallback to all lines for that app\n",
    "        probs = [y_pred_prob[i] for i, a in enumerate(idx_to_app) if a == app]\n",
    "    mean_prob = np.mean(probs, axis=0)\n",
    "    app_level_pred.append(int(np.argmax(mean_prob)))\n",
    "    app_level_true.append(int(app_true[app]))\n",
    "\n",
    "print(\"\\n[App-level from mean prob] Accuracy: %.4f  Macro-F1: %.4f\" %\n",
    "      (accuracy_score(app_level_true, app_level_pred),\n",
    "       f1_score(app_level_true, app_level_pred, average=\"macro\")))\n",
    "print(\"\\n[App-level] Classification Report\\n\",\n",
    "      classification_report(app_level_true, app_level_pred, target_names=list(le.classes_)))\n",
    "print(\"\\n[App-level] Confusion Matrix\\n\",\n",
    "      pd.DataFrame(confusion_matrix(app_level_true, app_level_pred),\n",
    "                   index=[f\"true_{c}\" for c in le.classes_],\n",
    "                   columns=[f\"pred_{c}\" for c in le.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a1319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:50:37.868696Z",
     "iopub.status.busy": "2025-09-21T20:50:37.868421Z",
     "iopub.status.idle": "2025-09-21T20:50:37.954672Z",
     "shell.execute_reply": "2025-09-21T20:50:37.953969Z"
    },
    "papermill": {
     "duration": 0.123637,
     "end_time": "2025-09-21T20:50:37.955823",
     "exception": false,
     "start_time": "2025-09-21T20:50:37.832186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved artifacts to /kaggle/working/:\n",
      " - line_charcnn_lstm.keras (best checkpoint)\n",
      " - line_charcnn_lstm_final.keras (final)\n",
      " - textvectorization_char.pkl\n",
      " - label_encoder_line.joblib\n",
      " - train_label_counts.csv / val_label_counts.csv / test_label_counts.csv\n"
     ]
    }
   ],
   "source": [
    "model.save(OUT_DIR / \"line_charcnn_lstm_final.keras\")\n",
    "\n",
    "vec_config = vec.get_config()\n",
    "vec_weights = vec.get_weights()\n",
    "joblib.dump({\"config\": vec_config, \"weights\": vec_weights}, OUT_DIR / \"textvectorization_char.pkl\")\n",
    "\n",
    "\n",
    "joblib.dump(le, OUT_DIR / \"label_encoder_line.joblib\")\n",
    "\n",
    "print(\"\\nSaved artifacts to /kaggle/working/:\")\n",
    "print(\" - line_charcnn_lstm.keras (best checkpoint)\")\n",
    "print(\" - line_charcnn_lstm_final.keras (final)\")\n",
    "print(\" - textvectorization_char.pkl\")\n",
    "print(\" - label_encoder_line.joblib\")\n",
    "print(\" - train_label_counts.csv / val_label_counts.csv / test_label_counts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465176e9",
   "metadata": {
    "papermill": {
     "duration": 0.034873,
     "end_time": "2025-09-21T20:50:38.026526",
     "exception": false,
     "start_time": "2025-09-21T20:50:37.991653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4864764,
     "sourceId": 8217270,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 209.315503,
   "end_time": "2025-09-21T20:50:41.665905",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-21T20:47:12.350402",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
